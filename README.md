# üß† GridWorld RL Project V3 : Comparaison DQN vs DQN DeepMind

## üéØ Description G√©n√©rale
Ce projet explore l‚Äôapprentissage par renforcement profond (*Deep Reinforcement Learning*) √† travers un environnement **GridWorld** personnalis√©.  
Deux agents sont entra√Æn√©s et compar√©s :

- **DQN Simple** : version de base du Deep Q-Learning.  
- **DQN DeepMind** : version avanc√©e int√©grant les principes de stabilisation d√©velopp√©s par **DeepMind** (utilisation d‚Äôun *Replay Buffer* et de deux r√©seaux neuronaux distincts : *Online* et *Target*).

L‚Äôobjectif est de mesurer l‚Äôimpact de ces m√©canismes sur la **vitesse d‚Äôapprentissage**, la **stabilit√©** et la **performance finale** des agents sur une vari√©t√© de sc√©narios.

---

## üó∫Ô∏è Contexte et Environnement
L‚Äôenvironnement **GridWorld** simule une grille 2D o√π l‚Äôagent doit atteindre un objectif tout en √©vitant des obstacles.

- Chaque √©pisode commence √† une position initiale et se termine lorsque l‚Äôagent atteint le but, tombe dans un pi√®ge ou d√©passe le nombre maximal d‚Äô√©tapes.  
- L‚Äôagent re√ßoit des **r√©compenses positives ou n√©gatives** selon ses actions.  
- Les grilles peuvent √™tre **g√©n√©r√©es al√©atoirement**, avec des **buts dynamiques** se d√©pla√ßant au fil du temps, for√ßant l‚Äôagent √† apprendre une **politique adaptative** et robuste.  

---

## ‚öôÔ∏è Philosophie et M√©thodologie
Ce projet met en parall√®le deux approches d‚Äôapprentissage par Q-Learning profond :

### üß© 1. DQN Simple
- Apprentissage direct des valeurs d‚Äôaction \( Q(s,a) \) via un seul r√©seau de neurones.  
- Mises √† jour effectu√©es √† chaque √©tape, sans m√©moire d‚Äôexp√©riences ni r√©seau secondaire.  
- Illustre les limites d‚Äôun DQN na√Øf : **instabilit√©**, **forte variance**, **apprentissage plus lent**.

### üî¨ 2. DQN DeepMind (Version am√©lior√©e)
Inspir√© du travail pionnier de **DeepMind (2015)** sur les jeux Atari :

- **Replay Buffer** : stocke des milliers de transitions `(√©tat, action, r√©compense, √©tat suivant)`, r√©√©chantillonn√©es de mani√®re al√©atoire pour casser la corr√©lation temporelle et stabiliser l‚Äôapprentissage.  
- **R√©seaux doubles (Online et Target)** :
  - Le r√©seau *Online* choisit les actions et s‚Äôentra√Æne.  
  - Le r√©seau *Target* est mis √† jour moins fr√©quemment pour fournir une cible d‚Äôapprentissage stable.  
- Ces m√©canismes r√©duisent l‚Äôinstabilit√© et permettent une **meilleure convergence** vers la politique optimale.

---

## üß™ Comparaison Exp√©rimentale
Le projet entra√Æne ces deux agents sur une **s√©rie de sc√©narios GridWorld g√©n√©r√©s al√©atoirement** :

- Chaque sc√©nario pr√©sente une configuration unique : taille de la grille, nombre d‚Äôobstacles, position du but.  
- L‚Äô√©valuation compare la **courbe d‚Äôapprentissage** (r√©compense cumul√©e, dur√©e des √©pisodes, stabilit√© de la perte) et la **politique finale** (chemin appris vers le but).  
- Un **r√©sum√© global** agr√®ge les performances de tous les sc√©narios pour visualiser les tendances g√©n√©rales.

---

## üìä Analyse et Visualisation
Le syst√®me de visualisation g√©n√®re automatiquement plusieurs graphiques :

- **Courbes d‚Äôapprentissage par sc√©nario** : √©volution des r√©compenses, du nombre d‚Äô√©tapes et de la fonction de perte.  
- **Trajectoires apprises** : repr√©sentation du chemin que chaque agent choisit en mode exploitation.  
- **R√©sum√© global (`global_summary_all_scenarios.png`)** : comparaison synth√©tique entre les deux approches (taux de r√©ussite, r√©compense moyenne, stabilit√©).  

Ces visualisations permettent d‚Äô√©valuer concr√®tement **l‚Äôefficacit√© de la stabilisation DeepMind** face √† un DQN simple.

---

## üìÅ Structure du Projet
üìÅ gridworld-rl-project-V3/
‚îú‚îÄ‚îÄ main_dqn.py # Script principal de comparaison et d‚Äôentra√Ænement
‚îú‚îÄ‚îÄ gridworld.py # Environnement GridWorld (h√©ritant de gym.Env)
‚îú‚îÄ‚îÄ agent_dqn.py # Agent DQN simple
‚îú‚îÄ‚îÄ agent_dqn_deepmind.py # Agent DQN am√©lior√© avec Replay Buffer et Target Network
‚îú‚îÄ‚îÄ results/ # Dossier g√©n√©r√© contenant les graphiques et comparaisons
‚îî‚îÄ‚îÄ README.md # Documentation du projet


---

## üöÄ Fonctionnement Global
Lancer le script `main_dqn.py` :

1. G√©n√®re automatiquement plusieurs sc√©narios GridWorld.  
2. Entra√Æne successivement les deux agents sur chaque sc√©nario.  
3. Sauvegarde les performances interm√©diaires et finales.  
4. Produit un ensemble complet de visualisations comparatives.  

L‚Äôexp√©rience met ainsi en √©vidence les **b√©n√©fices des principes de DeepMind** :  
> une convergence plus stable, un apprentissage plus rapide et des politiques mieux g√©n√©ralis√©es.

---

## üß≠ Interpr√©tation des R√©sultats (Optionnel)
- Le **DQN Simple** apprend plus vite au d√©part mais pr√©sente une forte instabilit√© et des performances finales variables.  
- Le **DQN DeepMind**, gr√¢ce √† son *Replay Buffer* et √† son *R√©seau Cible*, converge plus lentement au d√©but mais atteint une politique plus stable et efficace.  
- La variance de la r√©compense finale est significativement r√©duite, montrant l‚Äôeffet stabilisateur de la m√©thodologie DeepMind.  

---

## üõ†Ô∏è Installation
1. **Cloner le d√©p√¥t :**
```bash
git clone https://github.com/votre-nom-utilisateur/gridworld-rl-project-V3.git
cd gridworld-rl-project-V3
